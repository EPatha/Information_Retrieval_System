{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# belajar PREPROCESSING DATA, membuat Term Index\n",
    "#Step 1, Parsing => memisah dokumen\n",
    "#Step 2, Tokenisasi/Lexikal => memisah kata, hilangkan tanda baca dan, karakter\n",
    "#Step 3, Stopword Removal => hapus kata \"dan,yang, dll\"\n",
    "#Step 4, Pharse detector =>mencari kata duplikat untuk dihapus agar lebih efisien\n",
    "#Step 5, Stemming => menghapus kata imbuhan \"bermain\"->\"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama-nama kolom dalam DataFrame:\n",
      "['Unnamed: 0', 'Tweet', 'Sentimen', 'step01', 'tokens', 'final_tokens', 'step02']\n",
      "Beberapa baris dari DataFrame:\n",
      "   Unnamed: 0                                              Tweet  Sentimen  \\\n",
      "0           0  Cegah mata rantai Covid-19,mari kita dirumah s...       1.0   \n",
      "1           1  aku mohon yaAllah semoga wabah covid-19 menghi...       1.0   \n",
      "2           2  Pemprov Papua Naikkan Status Jadi Tanggap Daru...       1.0   \n",
      "3           3            Covid belum nyampe prigen mbak hmm hoax       0.0   \n",
      "4           4  Nyuruh orang pintar, lu aja Togog. Itu kerumun...      -1.0   \n",
      "5           5  Pikir2 balik byk mnde plk nk setelkn lepas covid.       0.0   \n",
      "6           6  Selamat pagi, hari jum'at. Jum'at keempat di k...       1.0   \n",
      "7           7  Hikmah di balik musibah covid-19, smg para pej...       1.0   \n",
      "8           8  Cegah covid-19 beserta jajaran Polsek Kuranji ...       1.0   \n",
      "9           9  Ya Allah kami memohon pada mu perkenankanlah d...       1.0   \n",
      "\n",
      "                                              step01  \\\n",
      "0  cegah mata rantai covid   mari kita dirumah sa...   \n",
      "1  aku mohon yaallah semoga wabah covid   menghil...   \n",
      "2  pemprov papua naikkan status jadi tanggap daru...   \n",
      "3            covid belum nyampe prigen mbak hmm hoax   \n",
      "4  nyuruh orang pintar lu aja togog itu kerumunan...   \n",
      "5    pikir balik byk mnde plk nk setelkn lepas covid   \n",
      "6  selamat pagi hari jumat jumat keempat di kala ...   \n",
      "7  hikmah di balik musibah covid   smg para pejab...   \n",
      "8  cegah covid   beserta jajaran polsek kuranji m...   \n",
      "9  ya allah kami memohon pada mu perkenankanlah d...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  ['cegah', 'mata', 'rantai', 'covid', 'mari', '...   \n",
      "1  ['aku', 'mohon', 'yaallah', 'semoga', 'wabah',...   \n",
      "2  ['pemprov', 'papua', 'naikkan', 'status', 'jad...   \n",
      "3  ['covid', 'belum', 'nyampe', 'prigen', 'mbak',...   \n",
      "4  ['nyuruh', 'orang', 'pintar', 'lu', 'aja', 'to...   \n",
      "5  ['pikir', 'balik', 'byk', 'mnde', 'plk', 'nk',...   \n",
      "6  ['selamat', 'pagi', 'hari', 'jumat', 'jumat', ...   \n",
      "7  ['hikmah', 'di', 'balik', 'musibah', 'covid', ...   \n",
      "8  ['cegah', 'covid', 'beserta', 'jajaran', 'pols...   \n",
      "9  ['ya', 'allah', 'kami', 'memohon', 'pada', 'mu...   \n",
      "\n",
      "                                        final_tokens  \\\n",
      "0  ['cegah', 'mata', 'rantai', 'covid', 'mari', '...   \n",
      "1  ['aku', 'mohon', 'yaallah', 'semoga', 'wabah',...   \n",
      "2  ['pemprov', 'papua', 'naikkan', 'status', 'jad...   \n",
      "3  ['covid', 'belum', 'nyampe', 'prigen', 'mbak',...   \n",
      "4  ['nyuruh', 'orang', 'pintar', 'lu', 'aja', 'to...   \n",
      "5  ['pikir', 'balik', 'byk', 'mnde', 'plk', 'nk',...   \n",
      "6  ['selamat', 'pagi', 'hari', 'jumat', 'jumat', ...   \n",
      "7  ['hikmah', 'di', 'balik', 'musibah', 'covid', ...   \n",
      "8  ['cegah', 'covid', 'beserta', 'jajaran', 'pols...   \n",
      "9  ['ya', 'allah', 'kami', 'memohon', 'pada', 'mu...   \n",
      "\n",
      "                                              step02  \n",
      "0  cegah mata rantai covid mari kita dirumah saja...  \n",
      "1  aku mohon yaallah semoga wabah covid menghilan...  \n",
      "2  pemprov papua naikkan status jadi tanggap daru...  \n",
      "3            covid belum nyampe prigen mbak hmm hoax  \n",
      "4  nyuruh orang pintar lu aja togog itu kerumunan...  \n",
      "5    pikir balik byk mnde plk nk setelkn lepas covid  \n",
      "6  selamat pagi hari jumat jumat keempat di kala ...  \n",
      "7  hikmah di balik musibah covid smg para pejabat...  \n",
      "8  cegah covid beserta jajaran polsek kuranji mel...  \n",
      "9  ya allah kami memohon pada mu perkenankanlah d...  \n",
      "Kolom 'content' tidak ditemukan dalam DataFrame.\n"
     ]
    }
   ],
   "source": [
    "#Package\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "#Step 1, Parsing => memisah dokumen\n",
    "\n",
    "# Baca file CSV\n",
    "file_path = '/home/sembarang/DATA/S1_Udinus/Matkul_Semester_5/Sistem_temu_kembali_informasi/Tugas_STKI1/clean_dataset.csv'\n",
    "df = pd.read_csv(file_path, on_bad_lines='skip', delimiter=';')\n",
    "\n",
    "# Tampilkan nama-nama kolom untuk pemeriksaan\n",
    "print(\"Nama-nama kolom dalam DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Tampilkan beberapa baris dari DataFrame untuk pengecekan\n",
    "print(\"Beberapa baris dari DataFrame:\")\n",
    "print(df.head(10))  # Tampilkan 10 baris pertama\n",
    "\n",
    "# Cek apakah kolom 'content' ada dalam DataFrame\n",
    "if 'content' not in df.columns:\n",
    "    print(\"Kolom 'content' tidak ditemukan dalam DataFrame.\")\n",
    "else:\n",
    "    parsed_data = []  # Inisialisasi parsed_data di sini\n",
    "\n",
    "    #Step 2, Tokenisasi/Lexikal => memisah kata, hilangkan tanda baca dan, karakter\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Cek apakah nilai dalam kolom 'content' tidak NaN dan tidak kosong\n",
    "        if pd.notna(row['content']) and row['content'].strip() != \"\":\n",
    "            content = row['content']\n",
    "            sentences = sent_tokenize(content)  # Tokenisasi kalimat\n",
    "            parsed_data.append(sentences)\n",
    "        else:\n",
    "            print(f\"Kolom 'content' kosong pada baris {index}.\")\n",
    "\n",
    "#Step 3, Stopword Removal => hapus kata \"dan,yang, dll\"\n",
    "    \n",
    "    # Hapus stopwords dari kalimat yang di-tokenisasi\n",
    "    stop_words = set(stopwords.words('indonesian'))  # Menggunakan stopwords bahasa Indonesia\n",
    "    custom_stopwords = {'dan', 'yang', 'ini'}  # Tambahkan kata-kata spesifik ke dalam stopwords\n",
    "    stop_words.update(custom_stopwords)  # Gabungkan dengan stopwords NLTK\n",
    "    \n",
    "#Step 4, Pharse detector =>mencari kata duplikat untuk dihapus agar lebih efisien\n",
    "\n",
    "#Step 5, Stemming => menghapus kata imbuhan \"bermain\"->\"main\"    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for index, sentences in enumerate(parsed_data):\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)  # Tokenisasi kata\n",
    "            # Hapus stopwords\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            # Lakukan stemming\n",
    "            stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "            # Hapus kata duplikat dengan mempertahankan urutan\n",
    "            unique_words = list(dict.fromkeys(stemmed_words))  # Mempertahankan urutan dan menghapus duplikat\n",
    "            # Gabungkan kembali kata-kata yang tersisa menjadi kalimat\n",
    "            cleaned_sentences.append(' '.join(unique_words))  # Gabungkan kata\n",
    "\n",
    "    # Simpan cleaned_sentences ke file CSV\n",
    "    cleaned_output_file_path = '/home/sembarang/DATA/S1_Udinus/Matkul_Semester_5/Sistem_temu_kembali_informasi/Tugas_STKI1/cleaned_sentences.csv'\n",
    "    cleaned_df = pd.DataFrame({'cleaned_sentences': cleaned_sentences})\n",
    "    cleaned_df.to_csv(cleaned_output_file_path, index=False)\n",
    "\n",
    "    print(f\"Hasil setelah menghapus stopwords, duplikat, dan melakukan stemming disimpan ke '{cleaned_output_file_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
